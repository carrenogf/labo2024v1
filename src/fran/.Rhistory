by = agrupa
]
}
ArbolEstimarGanancia <- function(semilla, param_basicos) {
# particiono estratificadamente el dataset
particionar(dataset, division = c(7, 3), agrupa = "clase_ternaria", seed = semilla)
# genero el modelo
# quiero predecir clase_ternaria a partir del resto
modelo <- rpart("clase_binaria1 ~ .",
data = dataset[fold == 1, !("clase_ternaria"), with = FALSE], # fold==1  es training,  el 70% de los datos
xval = 0,
control = param_basicos
) # aqui van los parametros del arbol
# aplico el modelo a los datos de testing
prediccion <- predict(modelo, # el modelo que genere recien
dataset[fold == 2, !("clase_ternaria"), with = FALSE], # fold==2  es testing, el 30% de los datos
type = "prob"
) # type= "prob"  es que devuelva la probabilidad
# prediccion es una matriz con TRES columnas,
#  llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
# cada columna es el vector de probabilidades
# calculo la ganancia en testing  qu es fold==2
ganancia_test <- dataset[
fold == 2,
sum(ifelse(prediccion[, "SUMA"] > 0.025,
ifelse(clase_binaria1 == "SUMA", 117000, -3000),
0
))
]
# escalo la ganancia como si fuera todo el dataset
ganancia_test_normalizada <- ganancia_test / 0.3
return(ganancia_test_normalizada)
}
ArbolesMontecarlo <- function(semillas, param_basicos) {
# la funcion mcmapply  llama a la funcion ArbolEstimarGanancia
#  tantas veces como valores tenga el vector  PARAM$semillas
ganancias <- mcmapply(ArbolEstimarGanancia,
semillas, # paso el vector de semillas
MoreArgs = list(param_basicos), # aqui paso el segundo parametro
SIMPLIFY = FALSE,
mc.cores = 5 # en Windows este valor debe ser 1
)
ganancia_promedio <- mean(unlist(ganancias))
return(ganancia_promedio)
}
# Aqui se debe poner la carpeta de la computadora local
#setwd("~/buckets/b1/") # Establezco el Working Directory
setwd("~/labo2024v1/src/fran")
# cargo los datos
dataset <- fread("https://storage.googleapis.com/open-courses/austral2024-fc72/dataset_pequeno.csv")
#dataset <- fread("./datasets/dataset_pequeno.csv")
# trabajo solo con los datos con clase, es decir 202107
dataset <- dataset[clase_ternaria != ""]
dataset[, clase_binaria1 := ifelse(clase_ternaria == "BAJA+2", "SUMA", "RESTA")]
# genero el archivo para Kaggle
# creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create("./exp/", showWarnings = FALSE)
dir.create("./exp/HT2020/", showWarnings = FALSE)
archivo_salida <- "./exp/HT2020/gridsearch_binaria1_2.txt"
# genero la data.table donde van los resultados del Grid Search
tb_grid_search <- data.table( max_depth = integer(),
min_split = integer(),
minbucket = integer(),
ganancia_promedio = numeric() )
md = c( 7, 8, 9, 10) # max_depeh
ms =  c(1600,1400,1200,1000, 800, 600, 400) #min_split
mb = seq(0.05,0.5,0.05) #minbucket
l <- 1
qmodelos <- length(md)*length(ms)*length(mb)
for (vmax_depth in md) {
for (vmin_split in ms) {
for (vminbucket in mb) {
# notar como se agrega
# vminsplit  minima cantidad de registros en un nodo para hacer el split
param_basicos <- list(
"cp" = -0.5, # complejidad minima
"minsplit" = vmin_split,
"minbucket" = vminbucket*vmin_split, # minima cantidad de registros en una hoja
"maxdepth" = vmax_depth
) # profundidad máxima del arbol
# Un solo llamado, con la semilla 17
ganancia_promedio <- ArbolesMontecarlo(PARAM$semillas, param_basicos)
# agrego a la tabla
tb_grid_search <- rbindlist(
list( tb_grid_search,
list( vmax_depth, vmin_split,vminbucket, ganancia_promedio) ) )
print(l," de ",qmodelos)
l <- l+1
}
}
# escribo la tabla a disco en cada vuelta del loop mas externo
Sys.sleep(2)  # espero un par de segundos
fwrite( tb_grid_search,
file = archivo_salida,
sep = "\t" )
}
print(qmodelos)
print(qmodelos+ "ss")
for (vmax_depth in md) {
for (vmin_split in ms) {
for (vminbucket in mb) {
# notar como se agrega
# vminsplit  minima cantidad de registros en un nodo para hacer el split
param_basicos <- list(
"cp" = -0.5, # complejidad minima
"minsplit" = vmin_split,
"minbucket" = vminbucket*vmin_split, # minima cantidad de registros en una hoja
"maxdepth" = vmax_depth
) # profundidad máxima del arbol
# Un solo llamado, con la semilla 17
ganancia_promedio <- ArbolesMontecarlo(PARAM$semillas, param_basicos)
# agrego a la tabla
tb_grid_search <- rbindlist(
list( tb_grid_search,
list( vmax_depth, vmin_split,vminbucket, ganancia_promedio) ) )
print(paste(l," de ",qmodelos))
l <- l+1
}
}
# escribo la tabla a disco en cada vuelta del loop mas externo
Sys.sleep(2)  # espero un par de segundos
fwrite( tb_grid_search,
file = archivo_salida,
sep = "\t" )
}
print("listo")
require( "data.table" )
require( "data.table" )
# reemplazar aqui por SUS semillas
mis_semillas <- c(606323, 368947, 251353, 259841, 764293)
tabla_semillas <- as.data.table(list( semilla = mis_semillas ))
fwrite( tabla_semillas,
file = "~/buckets/b1/datasets/mis_semillas.txt",
sep = "\t"
)
# limpio la memoria
rm(list = ls()) # remove all objects
gc() # garbage collection
require("data.table")
require("rlist")
require("rpart")
require("parallel")
# paquetes necesarios para la Bayesian Optimization
require("DiceKriging")
require("mlrMBO")
# cantidad de iteraciones de la Optimizacion Bayesiana
PARAM <- list()
PARAM$BO_iter <- 50 #cantidad de iteraciones de la Bayesian Optimization
# la letra L al final de 1L significa ENTERO
PARAM$hs <- makeParamSet(
makeNumericParam("cp", lower = -1, upper = 0.1),
makeIntegerParam("minsplit", lower = 1L, upper = 8000L),
makeIntegerParam("minbucket", lower = 1L, upper = 4000L),
makeIntegerParam("maxdepth", lower = 3L, upper = 20L),
forbidden = quote(minbucket > 0.5 * minsplit)
)
loguear <- function(reg, arch = NA, folder = "./work/", ext = ".txt",
verbose = TRUE) {
archivo <- arch
if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)
# Escribo los titulos
if (!file.exists(archivo)) {
linea <- paste0(
"fecha\t",
paste(list.names(reg), collapse = "\t"), "\n"
)
cat(linea, file = archivo)
}
# la fecha y hora
linea <- paste0(
format(Sys.time(), "%Y%m%d %H%M%S"), "\t",
gsub(", ", "\t", toString(reg)), "\n"
)
# grabo al archivo
cat(linea, file = archivo, append = TRUE)
# imprimo por pantalla
if (verbose) cat(linea)
}
particionar <- function(data, division, agrupa = "", campo = "fold",
start = 1, seed = NA) {
if (!is.na(seed)) set.seed(seed)
bloque <- unlist(mapply(
function(x, y) {
rep(y, x)
}, division,
seq(from = start, length.out = length(division))
))
data[, (campo) := sample(rep(bloque, ceiling(.N / length(bloque))))[1:.N],
by = agrupa
]
}
ArbolSimple <- function(fold_test, data, param) {
# genero el modelo
# entreno en todo MENOS el fold_test que uso para testing
modelo <- rpart("clase_ternaria ~ .",
data = data[fold != fold_test, ],
xval = 0,
control = param
)
# aplico el modelo a los datos de testing
# aplico el modelo sobre los datos de testing
# quiero que me devuelva probabilidades
prediccion <- predict(modelo,
data[fold == fold_test, ],
type = "prob"
)
# esta es la probabilidad de baja
prob_baja2 <- prediccion[, "BAJA+2"]
# calculo la ganancia
ganancia_testing <- data[fold == fold_test][
prob_baja2 > 1 / 40,
sum(ifelse(clase_ternaria == "BAJA+2",
117000, -3000
))
]
# esta es la ganancia sobre el fold de testing, NO esta normalizada
return(ganancia_testing)
}
ArbolesCrossValidation <- function(data, param, qfolds, pagrupa, semilla) {
# generalmente  c(1, 1, 1, 1, 1 )  cinco unos
divi <- rep(1, qfolds)
# particiono en dataset en folds
particionar(data, divi, seed = semilla, agrupa = pagrupa)
ganancias <- mcmapply(ArbolSimple,
seq(qfolds), # 1 2 3 4 5
MoreArgs = list(data, param),
SIMPLIFY = FALSE,
mc.cores = qfolds
)
data[, fold := NULL]
# devuelvo la primer ganancia y el promedio
# promedio las ganancias
ganancia_promedio <- mean(unlist(ganancias))
# aqui normalizo la ganancia
ganancia_promedio_normalizada <- ganancia_promedio * qfolds
return(ganancia_promedio_normalizada)
}
EstimarGanancia <- function(x) {
GLOBAL_iteracion <<- GLOBAL_iteracion + 1
xval_folds <- 5
# param= x los hiperparametros del arbol
# qfolds= xval_folds  la cantidad de folds
ganancia <- ArbolesCrossValidation(dataset,
param = x,
qfolds = xval_folds,
pagrupa = "clase_ternaria",
semilla = ksemilla_azar
)
# logueo
xx <- x
xx$xval_folds <- xval_folds
xx$ganancia <- ganancia
xx$iteracion <- GLOBAL_iteracion
# si es ganancia superadora la almaceno en mejor
if( ganancia > GLOBAL_mejor ) {
GLOBAL_mejor <<- ganancia
Sys.sleep(2)
loguear(xx, arch = archivo_log_mejor)
}
Sys.sleep(2)
loguear(xx, arch = archivo_log)
return(ganancia)
}
# Establezco el Working Directory
setwd("~/buckets/b1/")
#cargo MI semilla, que esta en MI bucket
tabla_semillas <- fread( "./datasets//mis_semillas.txt" )
ksemilla_azar <- tabla_semillas[ 1, semilla ]  # 1 es mi primer semilla
# cargo los datos
dataset <- fread("./datasets/dataset_pequeno.csv")
# creo la carpeta donde va el experimento
#  HT  representa  Hiperparameter Tuning
dir.create("./exp/", showWarnings = FALSE)
dir.create("./exp/HT3210/", showWarnings = FALSE)
# Establezco el Working Directory DEL EXPERIMENTO
setwd("./exp/HT3210/")
archivo_log <- "HT321.txt"
archivo_log_mejor <- "HT321_mejor.txt"
archivo_BO <- "HT321.RDATA"
# leo si ya existe el log
#  para retomar en caso que se se corte el programa
GLOBAL_iteracion <- 0
GLOBAL_mejor <- -Inf
if (file.exists(archivo_log)) {
tabla_log <- fread(archivo_log)
GLOBAL_iteracion <- nrow(tabla_log)
GLOBAL_mejor <- tabla_log[, max(ganancia)]
}
funcion_optimizar <- EstimarGanancia
configureMlr(show.learner.output = FALSE)
# configuro la busqueda bayesiana,
#  los hiperparametros que se van a optimizar
# por favor, no desesperarse por lo complejo
# minimize= FALSE estoy Maximizando la ganancia
obj.fun <- makeSingleObjectiveFunction(
fn = funcion_optimizar,
minimize = FALSE,
noisy = TRUE,
par.set = PARAM$hs,
has.simple.signature = FALSE
)
ctrl <- makeMBOControl(
save.on.disk.at.time = 600,
save.file.path = archivo_BO
)
ctrl <- setMBOControlTermination(ctrl, iters = PARAM$BO_iter)
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())
surr.km <- makeLearner("regr.km",
predict.type = "se",
covtype = "matern3_2", control = list(trace = TRUE)
)
# inicio la optimizacion bayesiana
if (!file.exists(archivo_BO)) {
run <- mbo(
fun = obj.fun,
learner = surr.km,
control = ctrl
)
} else {
run <- mboContinue(archivo_BO)
}
# limpio la memoria
rm(list = ls()) # remove all objects
gc() # garbage collection
require("data.table")
require("rlist")
require("rpart")
require("parallel")
# paquetes necesarios para la Bayesian Optimization
require("DiceKriging")
require("mlrMBO")
# cantidad de iteraciones de la Optimizacion Bayesiana
PARAM <- list()
PARAM$BO_iter <- 50 #cantidad de iteraciones de la Bayesian Optimization
# la letra L al final de 1L significa ENTERO
PARAM$hs <- makeParamSet(
makeNumericParam("cp", lower = -1, upper = 0.1),
makeIntegerParam("minsplit", lower = 1L, upper = 8000L),
makeIntegerParam("minbucket", lower = 1L, upper = 4000L),
makeIntegerParam("maxdepth", lower = 3L, upper = 20L),
forbidden = quote(minbucket > 0.5 * minsplit)
)
loguear <- function(reg, arch = NA, folder = "./work/", ext = ".txt",
verbose = TRUE) {
archivo <- arch
if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)
# Escribo los titulos
if (!file.exists(archivo)) {
linea <- paste0(
"fecha\t",
paste(list.names(reg), collapse = "\t"), "\n"
)
cat(linea, file = archivo)
}
# la fecha y hora
linea <- paste0(
format(Sys.time(), "%Y%m%d %H%M%S"), "\t",
gsub(", ", "\t", toString(reg)), "\n"
)
# grabo al archivo
cat(linea, file = archivo, append = TRUE)
# imprimo por pantalla
if (verbose) cat(linea)
}
particionar <- function(data, division, agrupa = "", campo = "fold",
start = 1, seed = NA) {
if (!is.na(seed)) set.seed(seed)
bloque <- unlist(mapply(
function(x, y) {
rep(y, x)
}, division,
seq(from = start, length.out = length(division))
))
data[, (campo) := sample(rep(bloque, ceiling(.N / length(bloque))))[1:.N],
by = agrupa
]
}
ArbolSimple <- function(fold_test, data, param) {
# genero el modelo
# entreno en todo MENOS el fold_test que uso para testing
modelo <- rpart("clase_ternaria ~ .",
data = data[fold != fold_test, ],
xval = 0,
control = param
)
# aplico el modelo a los datos de testing
# aplico el modelo sobre los datos de testing
# quiero que me devuelva probabilidades
prediccion <- predict(modelo,
data[fold == fold_test, ],
type = "prob"
)
# esta es la probabilidad de baja
prob_baja2 <- prediccion[, "BAJA+2"]
# calculo la ganancia
ganancia_testing <- data[fold == fold_test][
prob_baja2 > 1 / 40,
sum(ifelse(clase_ternaria == "BAJA+2",
117000, -3000
))
]
# esta es la ganancia sobre el fold de testing, NO esta normalizada
return(ganancia_testing)
}
ArbolesCrossValidation <- function(data, param, qfolds, pagrupa, semilla) {
# generalmente  c(1, 1, 1, 1, 1 )  cinco unos
divi <- rep(1, qfolds)
# particiono en dataset en folds
particionar(data, divi, seed = semilla, agrupa = pagrupa)
ganancias <- mcmapply(ArbolSimple,
seq(qfolds), # 1 2 3 4 5
MoreArgs = list(data, param),
SIMPLIFY = FALSE,
mc.cores = qfolds
)
data[, fold := NULL]
# devuelvo la primer ganancia y el promedio
# promedio las ganancias
ganancia_promedio <- mean(unlist(ganancias))
# aqui normalizo la ganancia
ganancia_promedio_normalizada <- ganancia_promedio * qfolds
return(ganancia_promedio_normalizada)
}
EstimarGanancia <- function(x) {
GLOBAL_iteracion <<- GLOBAL_iteracion + 1
xval_folds <- 5
# param= x los hiperparametros del arbol
# qfolds= xval_folds  la cantidad de folds
ganancia <- ArbolesCrossValidation(dataset,
param = x,
qfolds = xval_folds,
pagrupa = "clase_ternaria",
semilla = ksemilla_azar
)
# logueo
xx <- x
xx$xval_folds <- xval_folds
xx$ganancia <- ganancia
xx$iteracion <- GLOBAL_iteracion
# si es ganancia superadora la almaceno en mejor
if( ganancia > GLOBAL_mejor ) {
GLOBAL_mejor <<- ganancia
Sys.sleep(2)
loguear(xx, arch = archivo_log_mejor)
}
Sys.sleep(2)
loguear(xx, arch = archivo_log)
return(ganancia)
}
# Establezco el Working Directory
setwd("~/buckets/b1/")
#cargo MI semilla, que esta en MI bucket
tabla_semillas <- fread( "./datasets//mis_semillas.txt" )
ksemilla_azar <- tabla_semillas[ 1, semilla ]  # 1 es mi primer semilla
# cargo los datos
dataset <- fread("./datasets/dataset_pequeno.csv")
# entreno en 202107
dataset <- dataset[foto_mes==202107]
# creo la carpeta donde va el experimento
#  HT  representa  Hiperparameter Tuning
dir.create("./exp/", showWarnings = FALSE)
dir.create("./exp/HT3210/", showWarnings = FALSE)
# Establezco el Working Directory DEL EXPERIMENTO
setwd("./exp/HT3210/")
archivo_log <- "HT321.txt"
archivo_log_mejor <- "HT321_mejor.txt"
archivo_BO <- "HT321.RDATA"
# leo si ya existe el log
#  para retomar en caso que se se corte el programa
GLOBAL_iteracion <- 0
GLOBAL_mejor <- -Inf
if (file.exists(archivo_log)) {
tabla_log <- fread(archivo_log)
GLOBAL_iteracion <- nrow(tabla_log)
GLOBAL_mejor <- tabla_log[, max(ganancia)]
}
funcion_optimizar <- EstimarGanancia
configureMlr(show.learner.output = FALSE)
# configuro la busqueda bayesiana,
#  los hiperparametros que se van a optimizar
# por favor, no desesperarse por lo complejo
# minimize= FALSE estoy Maximizando la ganancia
obj.fun <- makeSingleObjectiveFunction(
fn = funcion_optimizar,
minimize = FALSE,
noisy = TRUE,
par.set = PARAM$hs,
has.simple.signature = FALSE
)
ctrl <- makeMBOControl(
save.on.disk.at.time = 600,
save.file.path = archivo_BO
)
ctrl <- setMBOControlTermination(ctrl, iters = PARAM$BO_iter)
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())
surr.km <- makeLearner("regr.km",
predict.type = "se",
covtype = "matern3_2", control = list(trace = TRUE)
)
# inicio la optimizacion bayesiana
if (!file.exists(archivo_BO)) {
run <- mbo(
fun = obj.fun,
learner = surr.km,
control = ctrl
)
} else {
run <- mboContinue(archivo_BO)
}
print("listo")
setwd("~/labo2024v1/src/fran")
